{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 加载谷歌硬盘\n"
      ],
      "metadata": {
        "id": "CvzE3ezBiOuW"
      },
      "id": "CvzE3ezBiOuW"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FbKOKq-XeZP",
        "outputId": "052a302a-55b4-4290-84b9-61761feda111"
      },
      "id": "-FbKOKq-XeZP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安装运行环境"
      ],
      "metadata": {
        "id": "o4WKBk0biT3i"
      },
      "id": "o4WKBk0biT3i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下内容是安装运行环境"
      ],
      "metadata": {
        "id": "fBmlfJBNX5Q6"
      },
      "id": "fBmlfJBNX5Q6"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update \\\n",
        " && sudo apt-get install -y \\\n",
        "  libxcb-icccm4 -y \\\n",
        "  libxcb-image0 -y \\\n",
        "  libxcb-keysyms1 -y \\\n",
        "  libxcb-randr0 -y \\\n",
        "  libxcb-render-util0 -y \\\n",
        "  libxcb-shape0 -y \\\n",
        "  libxcb-xfixes0 -y \\\n",
        "  libxcb-xinerama0 -y \\\n",
        "  libxcb-xkb1 -y \\\n",
        "  libxkbcommon-x11-0 -y\\\n",
        "  ffmpeg -y\\\n",
        "  libsm6 -y\\\n",
        "  libxext6 -y\\\n",
        "  freeglut3-dev\\\n",
        " && sudo rm -rf /var/lib/apt/lists/* "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahF7P5DIYE3w",
        "outputId": "fea5032c-a8c0-415a-dbaf-03c22b103a57"
      },
      "id": "ahF7P5DIYE3w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,069 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,299 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,972 kB]\n",
            "Hit:17 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,000 kB]\n",
            "Fetched 7,679 kB in 2s (4,469 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1).\n",
            "libxcb-icccm4 is already the newest version (0.4.1-1.1).\n",
            "libxcb-icccm4 set to manually installed.\n",
            "libxcb-image0 is already the newest version (0.4.0-1build1).\n",
            "libxcb-image0 set to manually installed.\n",
            "libxcb-keysyms1 is already the newest version (0.4.0-1build1).\n",
            "libxcb-keysyms1 set to manually installed.\n",
            "libxcb-randr0 is already the newest version (1.14-2).\n",
            "libxcb-randr0 set to manually installed.\n",
            "libxcb-render-util0 is already the newest version (0.3.9-1build1).\n",
            "libxcb-render-util0 set to manually installed.\n",
            "libxcb-shape0 is already the newest version (1.14-2).\n",
            "libxcb-shape0 set to manually installed.\n",
            "libxcb-xfixes0 is already the newest version (1.14-2).\n",
            "libxcb-xfixes0 set to manually installed.\n",
            "libxcb-xinerama0 is already the newest version (1.14-2).\n",
            "libxcb-xinerama0 set to manually installed.\n",
            "libxcb-xkb1 is already the newest version (1.14-2).\n",
            "libxcb-xkb1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-0ubuntu1).\n",
            "libxkbcommon-x11-0 is already the newest version (0.10.0-1).\n",
            "libxkbcommon-x11-0 set to manually installed.\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 freeglut3-dev\n",
            "0 upgraded, 2 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 198 kB of archives.\n",
            "After this operation, 1,078 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3-dev amd64 2.8.1-3 [124 kB]\n",
            "Fetched 198 kB in 0s (505 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 128208 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../freeglut3-dev_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpy \\\n",
        "pyqt5 \\\n",
        "pillow \\\n",
        "gym-retro \\\n",
        "gym==0.21.0 \\\n",
        "matplotlib \\\n",
        "stable-baselines3[extra] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUcNCSh_ZZS-",
        "outputId": "88096753-63d5-40ee-e33c-a3a991b29c5f"
      },
      "id": "LUcNCSh_ZZS-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.4)\n",
            "Collecting pyqt5\n",
            "  Downloading PyQt5-5.15.9-cp37-abi3-manylinux_2_17_x86_64.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n",
            "Collecting gym-retro\n",
            "  Downloading gym_retro-0.8.0-cp38-cp38-manylinux1_x86_64.whl (161.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.8/171.8 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.21.0) (2.2.1)\n",
            "Collecting PyQt5-sip<13,>=12.11\n",
            "  Downloading PyQt5_sip-12.11.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (361 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 KB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15.2\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyglet==1.*,>=1.3.2\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.13.1+cu116)\n",
            "Collecting importlib-metadata~=4.13\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (2.11.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.64.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Collecting ale-py==0.7.4\n",
            "  Downloading ale_py-0.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-13.3.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.5.tar.gz (22 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.51.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.38.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable-baselines3[extra]) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3[extra]) (2022.7.1)\n",
            "Collecting markdown-it-py<3.0.0,>=2.1.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.14.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.12.7)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=c6adf34ef7aeb1a28f3c0d76fe2305c3f644d98f454504c870c0087f71820890\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.5-py3-none-any.whl size=441098 sha256=8a30c663ad4bde6455df90ddf761107b4d6740f960ea0506a4a3f26eb96b5950\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/86/6f/e96885ff274388b9f0636418a2926f46f076cd7e891670321d\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: PyQt5-Qt5, pyglet, libtorrent, PyQt5-sip, pygments, mdurl, importlib-metadata, gym, pyqt5, markdown-it-py, gym-retro, AutoROM.accept-rom-license, autorom, ale-py, stable-baselines3, rich\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.5.5 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.1 ale-py-0.7.4 autorom-0.4.2 gym-0.21.0 gym-retro-0.8.0 importlib-metadata-4.13.0 libtorrent-2.0.7 markdown-it-py-2.2.0 mdurl-0.1.2 pyglet-1.5.27 pygments-2.14.0 pyqt5-5.15.9 rich-13.3.1 stable-baselines3-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入游戏ROM"
      ],
      "metadata": {
        "id": "RbNVd1Dkarhy"
      },
      "id": "RbNVd1Dkarhy"
    },
    {
      "cell_type": "code",
      "source": [
        "! python3  -m retro.import ./drive/MyDrive/Contra_train/Contra-Nes/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_0AbIQoYbco",
        "outputId": "56d84494-9d03-4f45-f009-a5fc2b881f86"
      },
      "id": "g_0AbIQoYbco",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing Contra-Nes\n",
            "Imported 1 games\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入运行库"
      ],
      "metadata": {
        "id": "eXxWk2h8avki"
      },
      "id": "eXxWk2h8avki"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import retro\n",
        "import gym\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from gym.wrappers import GrayScaleObservation\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "# 多线程\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "# 随机种子\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "import os\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "import numpy as np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "dw8vqhPuX3p3"
      },
      "id": "dw8vqhPuX3p3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "切换到硬盘目录"
      ],
      "metadata": {
        "id": "g-I3l3bXaxqV"
      },
      "id": "g-I3l3bXaxqV"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Contra_train/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ECPVDVa7i6",
        "outputId": "2dda8ab6-8e35-4720-e86f-ddcbc4a2ae9e"
      },
      "id": "A1ECPVDVa7i6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Contra_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"./Contra-Nes\"))\n",
        "\n",
        "retro.data.Integrations.add_custom_path(\n",
        "    os.path.join(SCRIPT_DIR, \"Contra-Nes/\")\n",
        ")\n",
        "rom_path = os.path.join(SCRIPT_DIR, \"Contra-Nes/\")\n",
        "\n",
        "\n",
        "CHECK_FREQ_NUMB = 10000\n",
        "TOTAL_TIMESTEP_NUMB = 2000000\n",
        "# LEARNING_RATE = 0.00005\n",
        "LEARNING_RATE = 0.000003\n",
        "N_STEPS = 2048\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 10\n",
        "DOWN_SAMPLE_RATE = 3\n",
        "SKIP_NUMB = 4\n",
        "EPISODE_NUMBERS = 20\n",
        "\n",
        "CHECKPOINT_DIR = './training/'\n",
        "LOG_DIR = './logs/'\n",
        "REWARD_LOG_FILENAME = 'reward_log.csv'\n",
        "\n",
        "with open(REWARD_LOG_FILENAME, 'a') as f:\n",
        "    print('timesteps,reward,bestreward', file=f)\n",
        "\n",
        "\n",
        "# 计算奖励函数\n",
        "class DeadlockEnv(gym.Wrapper):\n",
        "    def __init__(self, env, threshold=10):\n",
        "        super().__init__(env)\n",
        "        self.last_lives = 2\n",
        "        self.count = 0\n",
        "        self.threshold = threshold\n",
        "        self.last_xscroll = 0\n",
        "        self.score = 0\n",
        "        # 初始武器\n",
        "        self.Weapon = 0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.score = 0\n",
        "        self.last_lives = 2\n",
        "        self.count = 0\n",
        "        self.last_xscroll = 0\n",
        "        # 定义一个空的屏幕\n",
        "        self.previous_frame = np.zeros((74, 80, 1))\n",
        "        # 人物生成位置随机\n",
        "        numbers = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "        random_number = random.choice(numbers)\n",
        "        self.env.load_state(\"Level4-\"+random_number)\n",
        "        self.enemy_hp_list = [0] * 11\n",
        "\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, info = self.env.step(action)\n",
        "        reward = 0\n",
        "        # self.env.render()\n",
        "        # print(state.shape)\n",
        "        ifdie = info['die']\n",
        "        lives = info['lives']\n",
        "        score = info['score']\n",
        "        xpos = info['xpos']\n",
        "        xscroll = info['xscroll']\n",
        "        boss_defeated = info['beat_boss']\n",
        "        Weapon = info['Weapon']\n",
        "\n",
        "        if lives == 0:\n",
        "            reward = 0\n",
        "            done = True\n",
        "\n",
        "        # 打中敌人加分\n",
        "        for i in range(1, 10):\n",
        "            var_name = 'ehp' + str(i)\n",
        "            # print(var_name+\":\"+str(info[var_name]))\n",
        "            if info[var_name]>1 and info[var_name]<self.enemy_hp_list[i]: \n",
        "                # print(\"打中敌人加分+1\")\n",
        "                reward += 10\n",
        "                self.enemy_hp_list[i]=info[var_name]\n",
        "            elif info[var_name]>self.enemy_hp_list[i]:\n",
        "                self.enemy_hp_list[i]=info[var_name]\n",
        "\n",
        "        #分数提成 \n",
        "        if score > self.score:\n",
        "            reward += (score-self.score)/100\n",
        "            self.score = score\n",
        "        # 获取S弹加分\n",
        "        # Weapon 3 或者19 是s弹\n",
        "        if Weapon == 3 and Weapon != self.Weapon:\n",
        "            reward += 10\n",
        "            self.Weapon = Weapon\n",
        "        elif Weapon == 19 and Weapon != self.Weapon:\n",
        "            reward += 10\n",
        "            self.Weapon = Weapon\n",
        "        elif Weapon == 16 and Weapon != self.Weapon:\n",
        "            reward += 10\n",
        "            self.Weapon = Weapon\n",
        "\n",
        "        # Animation==85 被电了\n",
        "        if info['Animation'] == 85:\n",
        "            reward -= 0.01\n",
        "            # reward = 0\n",
        "            # done = True\n",
        "\n",
        "        # 防止卡死\n",
        "        if xscroll == self.last_xscroll:\n",
        "            if xscroll != 1208:\n",
        "                reward -= 0.01\n",
        "                self.count += 1\n",
        "            elif xscroll == 1208:\n",
        "                self.count = 0\n",
        "        else:\n",
        "            self.count = 0\n",
        "\t\t\t\t\n",
        "        if self.count >= 3200 :\n",
        "            reward = 0\n",
        "            print(\"超时了\")\n",
        "            done = True\n",
        "\n",
        "            # # 防止卡死\n",
        "        # if xscroll ==1208:\n",
        "        #     self.count += 1\n",
        "        # if self.count >= 8000:\n",
        "        #     # reward -= 10\n",
        "        #     done = True\n",
        "\n",
        "        # if xscroll == self.last_xscroll:\n",
        "        #     reward -= 0.01\n",
        "\n",
        "        if xscroll != self.last_xscroll:\n",
        "            # xscroll in [1, 2, 3, 4, 5]:\n",
        "            # if xscroll in [1, 2, 3, 4, 5]:\n",
        "            # \treward +=15\n",
        "            # else:\n",
        "            reward += 3\n",
        "            self.last_xscroll = xscroll\n",
        "\n",
        "        if ifdie != 1:\n",
        "            reward -= 0.01\n",
        "        # 通关得分\n",
        "        if boss_defeated == 8:\n",
        "            reward += 5000\n",
        "        # print(reward)\n",
        "        # time.sleep(0.01)\n",
        "        return state, reward, done, info\n",
        "\n",
        "# 跳过画面每2真保留一个画面，节省计算时间\n",
        "\n",
        "\n",
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        return obs, reward, done, info\n",
        "\n",
        "\n",
        "class Downsample(gym.ObservationWrapper):\n",
        "    def __init__(self, env, ratio):\n",
        "        \"\"\"\n",
        "        Downsample images by a factor of ratio\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        (oldh, oldw, oldc) = env.observation_space.shape\n",
        "        newshape = (oldh//ratio, oldw//ratio, oldc)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
        "                                                shape=newshape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        height, width, _ = self.observation_space.shape\n",
        "        frame = cv2.resize(frame, (width, height),\n",
        "                           interpolation=cv2.INTER_AREA)\n",
        "        if frame.ndim == 2:\n",
        "            frame = frame[:, :, None]\n",
        "        # 计算和上一帧的差值\n",
        "        frame_delta = frame - self.previous_frame\n",
        "        self.previous_frame = frame\n",
        "        # 将画面还原为普通的。方便人类观察\n",
        "        # result = np.where(frame_delta != 0, frame, 0)\n",
        "        # # 绘制画面\n",
        "        # cv2.imshow('game',np.array(result))\n",
        "        # key = cv2.waitKey(10)\n",
        "        return frame_delta\n",
        "\n",
        "\n",
        "class Discretizer(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Wrap a gym environment and make it use discrete actions.\n",
        "    Args:\n",
        "        combos: ordered list of lists of valid button combinations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, combos):\n",
        "        super().__init__(env)\n",
        "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
        "        buttons = env.unwrapped.buttons\n",
        "        self._decode_discrete_action = []\n",
        "        for combo in combos:\n",
        "            arr = np.array([False] * env.action_space.n)\n",
        "            for button in combo:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._decode_discrete_action.append(arr)\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(\n",
        "            len(self._decode_discrete_action))\n",
        "\n",
        "    def action(self, act):\n",
        "        return self._decode_discrete_action[act].copy()\n",
        "\n",
        "\n",
        "class SonicDiscretizer(Discretizer):\n",
        "    \"\"\"\n",
        "    Use Sonic-specific discrete actions\n",
        "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
        "    \"\"\"\n",
        "    # B是子弹\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env=env, combos=[['UP', 'B'], ['DOWN', 'B'], [\n",
        "            'LEFT', 'B'], ['RIGHT', 'B'], ['A'], ['B']])\n",
        "\n",
        "\n",
        "# env = retro.make(game='Contra-Nes' ,state=\"Level1\",record=\"./record/.\")\n",
        "\n",
        "\n",
        "env = retro.make(game=rom_path, state=\"Level4-0\",\n",
        "                 inttype=retro.data.Integrations.CUSTOM_ONLY)\n",
        "\n",
        "# # 人物生成位置随机\n",
        "# numbers = [0,1,2,3,4,5]\n",
        "# random_number = random.choice(numbers)\n",
        "# rom = env.data.set_value\n",
        "# rom(\"xscroll\", random_number)\n",
        "\n",
        "# 限制按键\n",
        "env = SonicDiscretizer(env)\n",
        "# 计算奖励函数\n",
        "env = DeadlockEnv(env)\n",
        "# 跳过一阵的画面\n",
        "env = SkipFrame(env, skip=SKIP_NUMB)\n",
        "monitor_dir = r'./monitor_log/'\n",
        "os.makedirs(monitor_dir, exist_ok=True)\n",
        "env = Monitor(env, monitor_dir)\n",
        "\n",
        "env = GrayScaleObservation(env, keep_dim=True)\n",
        "# 把画面的画质除以3\n",
        "# (224, 240, 1)\n",
        "# 变成\n",
        "# (74, 80, 1)\n",
        "env = Downsample(env, DOWN_SAMPLE_RATE)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "env = VecFrameStack(env, 4, channels_order='last')\n",
        "\n",
        "global best_score\n",
        "best_score = 0\n",
        "\n",
        "\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "    def __init__(self, check_freq, save_path, verbose=1):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def _init_callback(self):\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = os.path.join(\n",
        "                self.save_path, 'best_model_{}'.format(self.n_calls))\n",
        "            self.model.save(model_path)\n",
        "\n",
        "            total_reward = [0] * EPISODE_NUMBERS\n",
        "            total_time = [0] * EPISODE_NUMBERS\n",
        "            best_reward = 0\n",
        "            global best_score\n",
        "            for i in range(EPISODE_NUMBERS):\n",
        "                state = env.reset()  # reset for each new trial\n",
        "                done = False\n",
        "                total_reward[i] = 0\n",
        "                total_time[i] = 0\n",
        "                while not done and total_time[i] < 10000:\n",
        "                    action, _ = model.predict(state)\n",
        "                    last_state = state\n",
        "                    state, reward, done, info = env.step(action)\n",
        "                    total_reward[i] += reward[0]\n",
        "                    total_time[i] += 1\n",
        "\n",
        "                if total_reward[i] > best_reward:\n",
        "                    best_reward = total_reward[i]\n",
        "                    # 绘制画面\n",
        "                    if best_reward > best_score:\n",
        "                        best_score = best_reward\n",
        "                        cv2.imwrite('last3.png', np.array(\n",
        "                            last_state[0][:, :, 3]))\n",
        "                        cv2.imwrite('last2.png', np.array(\n",
        "                            last_state[0][:, :, 2]))\n",
        "                        cv2.imwrite('last1.png', np.array(\n",
        "                            last_state[0][:, :, 1]))\n",
        "                        cv2.imwrite('last0.png', np.array(\n",
        "                            last_state[0][:, :, 0]))\n",
        "                    best_epoch = self.n_calls\n",
        "\n",
        "                state = env.reset()  # reset for each new trial\n",
        "\n",
        "            print('time steps:', self.n_calls, '/', TOTAL_TIMESTEP_NUMB)\n",
        "            print('average reward:', (sum(total_reward) / EPISODE_NUMBERS),\n",
        "                  'average time:', (sum(total_time) / EPISODE_NUMBERS),\n",
        "                  'best_reward:', best_reward,\n",
        "                  )\n",
        "            with open(REWARD_LOG_FILENAME, 'a') as f:\n",
        "                print(self.n_calls, ',', sum(total_reward) /\n",
        "                      EPISODE_NUMBERS, ',', best_reward, file=f)\n",
        "        return True\n",
        "\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq: (int)\n",
        "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: (int)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, check_freq, save_model_dir, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_path = os.path.join(save_model_dir, 'best_model/')\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    # def _init_callback(self) -> None:\n",
        "    def _init_callback(self):\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    # def _on_step(self) -> bool:\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            print('self.n_calls: ', self.n_calls)\n",
        "            model_path1 = os.path.join(\n",
        "                self.save_path, 'model_{}'.format(self.n_calls))\n",
        "            self.model.save(model_path1)\n",
        "        return True\n",
        "\n",
        "\n",
        "callback = TrainAndLoggingCallback(\n",
        "    check_freq=CHECK_FREQ_NUMB, save_path=CHECKPOINT_DIR)\n",
        "# This is the AI model started\n",
        "# model = PPO('CnnPolicy', env, verbose=0, tensorboard_log=LOG_DIR, learning_rate=LEARNING_RATE, n_steps=N_STEPS,\n",
        "#             batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA)\n",
        "# 'learning_rate': 9.434717363652453e-05,\n",
        "\n",
        "model_params = {\n",
        "    'n_steps': 5952,\n",
        "    'gamma': 0.829362230935383,\n",
        "    # 'learning_rate': 8e-06,\n",
        "    'clip_range': 0.32876417393678736,\n",
        "    'gae_lambda': 0.8889831140956214\n",
        "}\n",
        "\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func\n",
        "\n",
        "\n",
        "lr_schedule = linear_schedule(1.12e-05)\n",
        "\n",
        "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR,\n",
        "            verbose=1, learning_rate=lr_schedule, **model_params)\n",
        "model.set_parameters(\"training/best_model_40000\")\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB,\n",
        "            callback=callback, reset_num_timesteps=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocevhl9ba0ta",
        "outputId": "8b174b64-d3f9-4d93-c93f-f8d2a4a5cd0b"
      },
      "id": "Ocevhl9ba0ta",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Logging to ./logs/PPO_12\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 695      |\n",
            "|    ep_rew_mean     | -6.36    |\n",
            "| time/              |          |\n",
            "|    fps             | 214      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 27       |\n",
            "|    total_timesteps | 5952     |\n",
            "---------------------------------\n",
            "超时了\n",
            "超时了\n",
            "time steps: 10000 / 2000000\n",
            "average reward: -3.678999918885529 average time: 508.6 best_reward: 17.499999597668648\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 674         |\n",
            "|    ep_rew_mean          | -6.15       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 109         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 108         |\n",
            "|    total_timesteps      | 11904       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004731396 |\n",
            "|    clip_fraction        | 0.0152      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.349      |\n",
            "|    explained_variance   | -0.00637    |\n",
            "|    learning_rate        | 1.12e-05    |\n",
            "|    loss                 | -0.0122     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00361    |\n",
            "|    value_loss           | 0.0706      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 611         |\n",
            "|    ep_rew_mean          | -4.26       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 144         |\n",
            "|    total_timesteps      | 17856       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014685349 |\n",
            "|    clip_fraction        | 0.0668      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.499      |\n",
            "|    explained_variance   | -4.65       |\n",
            "|    learning_rate        | 1.11e-05    |\n",
            "|    loss                 | -0.0244     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00769    |\n",
            "|    value_loss           | 0.00306     |\n",
            "-----------------------------------------\n",
            "time steps: 20000 / 2000000\n",
            "average reward: -5.7979998715221885 average time: 715.3 best_reward: 12.379999712109566\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 610          |\n",
            "|    ep_rew_mean          | -4.32        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 99           |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 239          |\n",
            "|    total_timesteps      | 23808        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040150583 |\n",
            "|    clip_fraction        | 0.0191       |\n",
            "|    clip_range           | 0.329        |\n",
            "|    entropy_loss         | -0.314       |\n",
            "|    explained_variance   | 0.138        |\n",
            "|    learning_rate        | 1.11e-05     |\n",
            "|    loss                 | 0.177        |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00937     |\n",
            "|    value_loss           | 0.114        |\n",
            "------------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 634        |\n",
            "|    ep_rew_mean          | -5         |\n",
            "| time/                   |            |\n",
            "|    fps                  | 108        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 275        |\n",
            "|    total_timesteps      | 29760      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00551134 |\n",
            "|    clip_fraction        | 0.0188     |\n",
            "|    clip_range           | 0.329      |\n",
            "|    entropy_loss         | -0.371     |\n",
            "|    explained_variance   | -0.642     |\n",
            "|    learning_rate        | 1.11e-05   |\n",
            "|    loss                 | -0.0352    |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.00984   |\n",
            "|    value_loss           | 0.0646     |\n",
            "----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "time steps: 30000 / 2000000\n",
            "average reward: -4.095499896537513 average time: 600.95 best_reward: 12.609999706968665\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 622         |\n",
            "|    ep_rew_mean          | -2.84       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 360         |\n",
            "|    total_timesteps      | 35712       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010176451 |\n",
            "|    clip_fraction        | 0.0298      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.449      |\n",
            "|    explained_variance   | -10.2       |\n",
            "|    learning_rate        | 1.1e-05     |\n",
            "|    loss                 | -0.0214     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.00779    |\n",
            "|    value_loss           | 0.00344     |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 40000 / 2000000\n",
            "average reward: -1.4979999807663262 average time: 595.2 best_reward: 47.40999868325889\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 649         |\n",
            "|    ep_rew_mean          | -3.42       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 93          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 444         |\n",
            "|    total_timesteps      | 41664       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006521443 |\n",
            "|    clip_fraction        | 0.0167      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.48       |\n",
            "|    explained_variance   | 0.0492      |\n",
            "|    learning_rate        | 1.1e-05     |\n",
            "|    loss                 | -0.00483    |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.00635    |\n",
            "|    value_loss           | 0.571       |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 658         |\n",
            "|    ep_rew_mean          | -3.68       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 481         |\n",
            "|    total_timesteps      | 47616       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009453095 |\n",
            "|    clip_fraction        | 0.0153      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.423      |\n",
            "|    explained_variance   | -0.101      |\n",
            "|    learning_rate        | 1.1e-05     |\n",
            "|    loss                 | -0.00896    |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00267    |\n",
            "|    value_loss           | 0.0377      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 50000 / 2000000\n",
            "average reward: -2.1009998679161073 average time: 658.25 best_reward: 57.93000042438507\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 658         |\n",
            "|    ep_rew_mean          | -3.68       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 93          |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 571         |\n",
            "|    total_timesteps      | 53568       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009547643 |\n",
            "|    clip_fraction        | 0.0236      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.78       |\n",
            "|    explained_variance   | -1.86       |\n",
            "|    learning_rate        | 1.09e-05    |\n",
            "|    loss                 | 0.0354      |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0115     |\n",
            "|    value_loss           | 0.0515      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 650         |\n",
            "|    ep_rew_mean          | -3.93       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 606         |\n",
            "|    total_timesteps      | 59520       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014590573 |\n",
            "|    clip_fraction        | 0.0252      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.823      |\n",
            "|    explained_variance   | -0.137      |\n",
            "|    learning_rate        | 1.09e-05    |\n",
            "|    loss                 | -0.0226     |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00975    |\n",
            "|    value_loss           | 0.0645      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 60000 / 2000000\n",
            "average reward: -9.421499789692461 average time: 950.0 best_reward: 7.119999835267663\n",
            "超时了\n",
            "超时了\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 665          |\n",
            "|    ep_rew_mean          | -4.27        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 90           |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 721          |\n",
            "|    total_timesteps      | 65472        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054975427 |\n",
            "|    clip_fraction        | 0.023        |\n",
            "|    clip_range           | 0.329        |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | -0.798       |\n",
            "|    learning_rate        | 1.09e-05     |\n",
            "|    loss                 | -0.0111      |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.00632     |\n",
            "|    value_loss           | 0.0462       |\n",
            "------------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 70000 / 2000000\n",
            "average reward: 1.229000030644238 average time: 681.2 best_reward: 127.70999855920672\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 686         |\n",
            "|    ep_rew_mean          | -4.65       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 87          |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 812         |\n",
            "|    total_timesteps      | 71424       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020814128 |\n",
            "|    clip_fraction        | 0.0459      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.632      |\n",
            "|    explained_variance   | -10.5       |\n",
            "|    learning_rate        | 1.08e-05    |\n",
            "|    loss                 | -0.00627    |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00343    |\n",
            "|    value_loss           | 0.00306     |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 691         |\n",
            "|    ep_rew_mean          | -4.65       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 91          |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 848         |\n",
            "|    total_timesteps      | 77376       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010675503 |\n",
            "|    clip_fraction        | 0.0251      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.727      |\n",
            "|    explained_variance   | -12.4       |\n",
            "|    learning_rate        | 1.08e-05    |\n",
            "|    loss                 | -0.0113     |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0068     |\n",
            "|    value_loss           | 0.00228     |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 80000 / 2000000\n",
            "average reward: -7.710499828774482 average time: 918.35 best_reward: 21.55999950133264\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 711         |\n",
            "|    ep_rew_mean          | -4.94       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 86          |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 959         |\n",
            "|    total_timesteps      | 83328       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011151785 |\n",
            "|    clip_fraction        | 0.0292      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.562      |\n",
            "|    explained_variance   | -0.0421     |\n",
            "|    learning_rate        | 1.08e-05    |\n",
            "|    loss                 | 0.0096      |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.00655    |\n",
            "|    value_loss           | 0.0689      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 712         |\n",
            "|    ep_rew_mean          | -4.74       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 89          |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 993         |\n",
            "|    total_timesteps      | 89280       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009998879 |\n",
            "|    clip_fraction        | 0.0199      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.567      |\n",
            "|    explained_variance   | -6.98       |\n",
            "|    learning_rate        | 1.07e-05    |\n",
            "|    loss                 | 0.0275      |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.00479    |\n",
            "|    value_loss           | 0.00219     |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 90000 / 2000000\n",
            "average reward: -4.2519999303855 average time: 820.05 best_reward: 27.64999913610518\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 740         |\n",
            "|    ep_rew_mean          | -5.03       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 86          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 1097        |\n",
            "|    total_timesteps      | 95232       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014917043 |\n",
            "|    clip_fraction        | 0.0321      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.563      |\n",
            "|    explained_variance   | -0.0953     |\n",
            "|    learning_rate        | 1.07e-05    |\n",
            "|    loss                 | -0.00365    |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    value_loss           | 0.11        |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 100000 / 2000000\n",
            "average reward: 1.1735000433400273 average time: 730.75 best_reward: 77.1099982317537\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 748        |\n",
            "|    ep_rew_mean          | -5         |\n",
            "| time/                   |            |\n",
            "|    fps                  | 84         |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 1192       |\n",
            "|    total_timesteps      | 101184     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00506102 |\n",
            "|    clip_fraction        | 0.0178     |\n",
            "|    clip_range           | 0.329      |\n",
            "|    entropy_loss         | -0.491     |\n",
            "|    explained_variance   | 0.00792    |\n",
            "|    learning_rate        | 1.07e-05   |\n",
            "|    loss                 | 0.722      |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.00311   |\n",
            "|    value_loss           | 0.146      |\n",
            "----------------------------------------\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 761         |\n",
            "|    ep_rew_mean          | -5.13       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 87          |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 1229        |\n",
            "|    total_timesteps      | 107136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011130501 |\n",
            "|    clip_fraction        | 0.0412      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.499      |\n",
            "|    explained_variance   | -0.00246    |\n",
            "|    learning_rate        | 1.06e-05    |\n",
            "|    loss                 | -0.0692     |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.00621    |\n",
            "|    value_loss           | 0.0738      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "超时了\n",
            "time steps: 110000 / 2000000\n",
            "average reward: -6.594499864894897 average time: 759.85 best_reward: 6.6199996173381805\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 774         |\n",
            "|    ep_rew_mean          | -4.77       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 85          |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 1326        |\n",
            "|    total_timesteps      | 113088      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007950646 |\n",
            "|    clip_fraction        | 0.0247      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.584      |\n",
            "|    explained_variance   | -0.0512     |\n",
            "|    learning_rate        | 1.06e-05    |\n",
            "|    loss                 | -0.0466     |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00828    |\n",
            "|    value_loss           | 0.0328      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 777         |\n",
            "|    ep_rew_mean          | -6          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 87          |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 1361        |\n",
            "|    total_timesteps      | 119040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008499393 |\n",
            "|    clip_fraction        | 0.0275      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.553      |\n",
            "|    explained_variance   | 0.0551      |\n",
            "|    learning_rate        | 1.06e-05    |\n",
            "|    loss                 | -0.0315     |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0122     |\n",
            "|    value_loss           | 0.102       |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "time steps: 120000 / 2000000\n",
            "average reward: -4.713499872107059 average time: 812.7 best_reward: 7.999999810010195\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 769         |\n",
            "|    ep_rew_mean          | -5.93       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 85          |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 1463        |\n",
            "|    total_timesteps      | 124992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010207366 |\n",
            "|    clip_fraction        | 0.0361      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.558      |\n",
            "|    explained_variance   | -0.665      |\n",
            "|    learning_rate        | 1.05e-05    |\n",
            "|    loss                 | -0.0162     |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0127     |\n",
            "|    value_loss           | 0.042       |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "time steps: 130000 / 2000000\n",
            "average reward: -6.3004998355172575 average time: 768.4 best_reward: 16.54000011458993\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 768         |\n",
            "|    ep_rew_mean          | -5.84       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 83          |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 1561        |\n",
            "|    total_timesteps      | 130944      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009250596 |\n",
            "|    clip_fraction        | 0.0299      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.579      |\n",
            "|    explained_variance   | -0.0463     |\n",
            "|    learning_rate        | 1.05e-05    |\n",
            "|    loss                 | 0.0309      |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.00699    |\n",
            "|    value_loss           | 0.0382      |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 776          |\n",
            "|    ep_rew_mean          | -6.11        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 85           |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 1597         |\n",
            "|    total_timesteps      | 136896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055165663 |\n",
            "|    clip_fraction        | 0.0243       |\n",
            "|    clip_range           | 0.329        |\n",
            "|    entropy_loss         | -0.426       |\n",
            "|    explained_variance   | -0.0395      |\n",
            "|    learning_rate        | 1.05e-05     |\n",
            "|    loss                 | 0.00509      |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00601     |\n",
            "|    value_loss           | 0.0742       |\n",
            "------------------------------------------\n",
            "超时了\n",
            "time steps: 140000 / 2000000\n",
            "average reward: -2.957999885268509 average time: 734.15 best_reward: 47.85999993048608\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 801         |\n",
            "|    ep_rew_mean          | -5.47       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 84          |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 1693        |\n",
            "|    total_timesteps      | 142848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005836148 |\n",
            "|    clip_fraction        | 0.0174      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.423      |\n",
            "|    explained_variance   | -8.53       |\n",
            "|    learning_rate        | 1.04e-05    |\n",
            "|    loss                 | -0.0189     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.00784    |\n",
            "|    value_loss           | 0.00241     |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 818         |\n",
            "|    ep_rew_mean          | -5.55       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 86          |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 1728        |\n",
            "|    total_timesteps      | 148800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009622558 |\n",
            "|    clip_fraction        | 0.0309      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.465      |\n",
            "|    explained_variance   | 0.0637      |\n",
            "|    learning_rate        | 1.04e-05    |\n",
            "|    loss                 | 0.0365      |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.00617    |\n",
            "|    value_loss           | 0.225       |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "time steps: 150000 / 2000000\n",
            "average reward: -1.2774998868815601 average time: 615.05 best_reward: 57.8800001591444\n",
            "超时了\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 781          |\n",
            "|    ep_rew_mean          | -4.99        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 85           |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 1815         |\n",
            "|    total_timesteps      | 154752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035019664 |\n",
            "|    clip_fraction        | 0.0178       |\n",
            "|    clip_range           | 0.329        |\n",
            "|    entropy_loss         | -0.364       |\n",
            "|    explained_variance   | -0.0457      |\n",
            "|    learning_rate        | 1.04e-05     |\n",
            "|    loss                 | -0.0116      |\n",
            "|    n_updates            | 250          |\n",
            "|    policy_gradient_loss | -0.0022      |\n",
            "|    value_loss           | 0.0382       |\n",
            "------------------------------------------\n",
            "time steps: 160000 / 2000000\n",
            "average reward: -5.345999893639236 average time: 820.25 best_reward: 23.299999233335257\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 754         |\n",
            "|    ep_rew_mean          | -4.73       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 83          |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 1918        |\n",
            "|    total_timesteps      | 160704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006961816 |\n",
            "|    clip_fraction        | 0.0199      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.398      |\n",
            "|    explained_variance   | -0.0479     |\n",
            "|    learning_rate        | 1.03e-05    |\n",
            "|    loss                 | -0.00589    |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.00429    |\n",
            "|    value_loss           | 0.075       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 758         |\n",
            "|    ep_rew_mean          | -4.78       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 85          |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 1953        |\n",
            "|    total_timesteps      | 166656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010729902 |\n",
            "|    clip_fraction        | 0.0283      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.466      |\n",
            "|    explained_variance   | -1.55       |\n",
            "|    learning_rate        | 1.03e-05    |\n",
            "|    loss                 | 0.035       |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    value_loss           | 0.088       |\n",
            "-----------------------------------------\n",
            "超时了\n",
            "超时了\n",
            "time steps: 170000 / 2000000\n",
            "average reward: -5.6099998754449185 average time: 699.05 best_reward: 4.6299998909235\n",
            "超时了\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 725          |\n",
            "|    ep_rew_mean          | -4.95        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 84           |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 2047         |\n",
            "|    total_timesteps      | 172608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067818696 |\n",
            "|    clip_fraction        | 0.0187       |\n",
            "|    clip_range           | 0.329        |\n",
            "|    entropy_loss         | -0.429       |\n",
            "|    explained_variance   | -0.285       |\n",
            "|    learning_rate        | 1.03e-05     |\n",
            "|    loss                 | -0.0279      |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.0112      |\n",
            "|    value_loss           | 0.0455       |\n",
            "------------------------------------------\n",
            "超时了\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 735         |\n",
            "|    ep_rew_mean          | -5.04       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 85          |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 2082        |\n",
            "|    total_timesteps      | 178560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006358772 |\n",
            "|    clip_fraction        | 0.0225      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.483      |\n",
            "|    explained_variance   | -1.45       |\n",
            "|    learning_rate        | 1.02e-05    |\n",
            "|    loss                 | -0.0389     |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0114     |\n",
            "|    value_loss           | 0.0481      |\n",
            "-----------------------------------------\n",
            "time steps: 180000 / 2000000\n",
            "average reward: -6.049999865051359 average time: 589.85 best_reward: 0\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 711         |\n",
            "|    ep_rew_mean          | -4.91       |\n",
            "| time/                   |             |\n",
            "|    fps                  | 85          |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 2167        |\n",
            "|    total_timesteps      | 184512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007740229 |\n",
            "|    clip_fraction        | 0.0274      |\n",
            "|    clip_range           | 0.329       |\n",
            "|    entropy_loss         | -0.358      |\n",
            "|    explained_variance   | -0.0657     |\n",
            "|    learning_rate        | 1.02e-05    |\n",
            "|    loss                 | 0.00772     |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.00549    |\n",
            "|    value_loss           | 0.074       |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Go8MKS0PnI",
        "outputId": "0af2a09d-4679-45b2-ade0-3a870f727d39"
      },
      "id": "u5Go8MKS0PnI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contra-Nes  last1.png  last3.png  monitor_log\t  tmp\n",
            "last0.png   last2.png  logs\t  reward_log.csv  training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.close() "
      ],
      "metadata": {
        "id": "yWWmM8Gidwvx"
      },
      "id": "yWWmM8Gidwvx",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}